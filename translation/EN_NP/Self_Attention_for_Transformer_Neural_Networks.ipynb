{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO5Z0qErNuOt"
      },
      "source": [
        "# Self Attention in Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HedntyUvLrBo"
      },
      "source": [
        "## Generate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xtKbaWhFJui3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: (4, 8), Key: (4, 8), Value: (4, 8)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "#Length of the sequence, dimension of the key, dimension of the value\n",
        "L, d_k, d_v = 4, 8, 8\n",
        "q = np.random.randn(L, d_k)\n",
        "k = np.random.randn(L, d_k)\n",
        "v = np.random.randn(L, d_v)\n",
        "print(f\"Query: {q.shape}, Key: {k.shape}, Value: {v.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09JpvuNJ2sZC",
        "outputId": "30d2c627-8647-44e0-aa92-c9e53e3b7843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q\n",
            " [[-0.74947248 -0.61467048  0.27795685 -1.52031796  0.19959513 -0.82411098\n",
            "   0.87743222 -0.10540908]\n",
            " [ 1.17429321 -1.84903911  0.04399508 -0.36574474  0.33059822 -0.30801206\n",
            "  -1.58653368  2.04158393]\n",
            " [ 1.03565886 -0.28076427 -0.02179129 -1.15421532  0.68699884  0.82653069\n",
            "  -0.34137593  1.10470612]\n",
            " [ 1.44176285  1.70663505 -0.38310084 -0.67941888 -0.20895877 -0.64475789\n",
            "   0.90452786 -1.93127615]]\n",
            "K\n",
            " [[ 0.34783044  0.33978026  1.84419648  0.1140325   0.43904318 -0.27640791\n",
            "   0.38455458 -1.42061539]\n",
            " [ 0.53235409  2.15349007 -0.6754666  -0.16133213  1.59204081  0.91056436\n",
            "   0.18703004 -0.68415461]\n",
            " [-0.5601536   1.25902356  0.06203232  1.20840015  0.37877804 -0.7393191\n",
            "  -0.72186826  0.23106176]\n",
            " [ 1.65577883  0.79578684 -1.07266697  0.43970972  0.4445189  -1.99685624\n",
            "   1.24360732 -0.30873901]]\n",
            "V\n",
            " [[-0.7368274  -0.61469973  0.32486237 -0.0570102   1.52021924  0.54076089\n",
            "   0.07458097 -0.51615085]\n",
            " [-0.04248102 -1.18526217 -0.8761295   1.10760605  2.20175399  1.31060859\n",
            "  -2.76460533 -1.70821689]\n",
            " [-0.64429528  1.04251321 -0.51087522  0.3928373   1.05181382 -0.81505056\n",
            "   0.51667582  0.16278935]\n",
            " [ 1.11096906  0.48076118 -0.35379643  0.48345357 -0.76334168  0.08578319\n",
            "   0.47118265  0.86716897]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV6txskBLwjh"
      },
      "source": [
        "## Self Attention\n",
        "\n",
        "$$\n",
        "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
        "$$\n",
        "$$\n",
        "M = Mask\n",
        "$$\n",
        "$$\n",
        "\\text{new V} = \\text{self attention}.V\n",
        "$$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7GePHKk3Mh0",
        "outputId": "7dae7f5e-4715-4fd4-fbfd-7c0815e7d39e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.67228716, -1.86156614, -2.1468383 ,  0.16131892],\n",
              "       [-3.46051378, -4.77508585, -1.45506242, -1.57641328],\n",
              "       [-1.53444764,  1.17434771, -2.17888867, -1.10343389],\n",
              "       [ 3.47529607,  5.38183481, -0.20535062,  6.77329111]])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.matmul(q, k.T) # (L, d_k) * (d_k, L) = (L, L) :(input_sequence_length, input_sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odK76OoI3nL2",
        "outputId": "69b50cdb-9a41-45ae-bfd2-619228af1ef7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.0064999976924682, 0.8923701905630524, 9.009341877886312)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Why we need sqrt(d_k) in denominator of softmax?\n",
        "# ANS: TO make the softmax more stable and to avoid saturation of softmax function\n",
        "# ANS: To normalize the variance of qk.T matrix \n",
        "q.var(), k.var(), np.matmul(q, k.T).var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ps6AY1Q3tRI",
        "outputId": "3b9ac3c8-70b8-47bd-e868-e7d6fd26d270"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.0064999976924682, 0.8923701905630524, 1.1261677347357888)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
        "q.var(), k.var(), scaled.var()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypO9IK1PL3cJ"
      },
      "source": [
        "Notice the reduction in variance of the product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVHAJR4N4VQX",
        "outputId": "52b06cf8-0381-453c-b576-0bd8de9a38b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.23768941, -0.65816302, -0.75902196,  0.05703485],\n",
              "       [-1.22347638, -1.68824779, -0.51444225, -0.55734626],\n",
              "       [-0.54250916,  0.41519461, -0.77035348, -0.39012279],\n",
              "       [ 1.22870271,  1.90276595, -0.07260241,  2.39472004]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmz4v-RmMAaj"
      },
      "source": [
        "## Masking\n",
        "\n",
        "- This is to ensure words don't get context from words generated in the future. \n",
        "- Not required in the encoders, but required in the decoders\n",
        "- Masking is done by setting the values to $-\\infty$ before the softmax\n",
        "- In Encoder, masking is done for padding\n",
        "- In Decoder, masking is done for padding (padding mask) and future words (look ahead mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8N3OhSLILfG",
        "outputId": "2c63a444-066c-44b2-abe5-242dd989f311"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [1., 1., 0., 0.],\n",
              "       [1., 1., 1., 0.],\n",
              "       [1., 1., 1., 1.]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# look ahead mask\n",
        "mask = np.tril(np.ones( (L, L) ))\n",
        "mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hIV9K3Yn6s1V"
      },
      "outputs": [],
      "source": [
        "mask[mask == 0] = -np.infty\n",
        "mask[mask == 1] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK5V_T3W6vpX",
        "outputId": "bb4160a1-a011-4850-e403-9cb252572c66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0., -inf, -inf, -inf],\n",
              "       [  0.,   0., -inf, -inf],\n",
              "       [  0.,   0.,   0., -inf],\n",
              "       [  0.,   0.,   0.,   0.]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNH1VgEf7xTa",
        "outputId": "4211c411-0356-4e39-8388-d39b0c1d0920"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.68537216,        -inf,        -inf,        -inf],\n",
              "       [ 0.47796088,  0.42358302,        -inf,        -inf],\n",
              "       [ 0.37611945, -0.30709922, -0.65849946,        -inf],\n",
              "       [ 0.78209275, -0.99700418,  1.88206279,  0.79213542]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaled + mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMTAXjooN9eZ"
      },
      "source": [
        "## Softmax\n",
        "\n",
        "$$\n",
        "\\text{softmax} = \\frac{e^{x_i}}{\\sum_j e^x_j}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2R4gdRqj8W4Y"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "K5eg2zPy41sP"
      },
      "outputs": [],
      "source": [
        "attention = softmax(scaled + mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sauNmfl-1TB",
        "outputId": "46b22beb-9034-4c7c-8d56-04209d2581c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.        , 0.        , 0.        , 0.        ],\n",
              "       [0.51359112, 0.48640888, 0.        , 0.        ],\n",
              "       [0.53753304, 0.27144826, 0.1910187 , 0.        ],\n",
              "       [0.19293995, 0.03256643, 0.57960627, 0.19488734]])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAy37go56LZo",
        "outputId": "78d97fa1-e0b3-4c1d-8294-bf0fdb77f199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.7368274 , -0.61469973,  0.32486237, -0.0570102 ,  1.52021924,\n",
              "         0.54076089,  0.07458097, -0.51615085],\n",
              "       [-0.46891071, -0.83485383, -0.13854577,  0.39236225,  1.78319251,\n",
              "         0.83781011, -1.02093191, -0.97611492],\n",
              "       [-0.30907621, -0.65267498, -0.53722892,  0.71374701,  1.8389214 ,\n",
              "         0.75122071, -1.52608719, -1.09897583],\n",
              "       [ 0.39881098, -0.17285712, -0.41416886,  0.58581643,  0.57065709,\n",
              "         0.49132587, -0.57380911, -0.16152413]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_v = np.matmul(attention, v)\n",
        "new_v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCP2aZOU9VrT",
        "outputId": "e1fe2137-cd95-4a4b-fa1a-3ec21c38104c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.7368274 , -0.61469973,  0.32486237, -0.0570102 ,  1.52021924,\n",
              "         0.54076089,  0.07458097, -0.51615085],\n",
              "       [-0.04248102, -1.18526217, -0.8761295 ,  1.10760605,  2.20175399,\n",
              "         1.31060859, -2.76460533, -1.70821689],\n",
              "       [-0.64429528,  1.04251321, -0.51087522,  0.3928373 ,  1.05181382,\n",
              "        -0.81505056,  0.51667582,  0.16278935],\n",
              "       [ 1.11096906,  0.48076118, -0.35379643,  0.48345357, -0.76334168,\n",
              "         0.08578319,  0.47118265,  0.86716897]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "h_JndWelLDNW"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4, 8)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSiJuBQELFHT"
      },
      "source": [
        "# Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XvTnmdcB_jdq"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "  d_k = q.shape[-1]\n",
        "  scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scaled = scaled + mask\n",
        "  attention = softmax(scaled)\n",
        "  out = np.matmul(attention, v)\n",
        "  return out, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSxLkZdiSLMT",
        "outputId": "ca70508d-fb6e-4eec-acb6-7a89a60dffa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q\n",
            " [[ 0.11672673 -2.54870451 -1.44065948  0.93661829  1.36278968  1.04252277\n",
            "  -0.01310938 -1.3163937 ]\n",
            " [ 0.26721599 -0.90218255  0.07417847 -0.10430246  0.52684253 -0.07081531\n",
            "  -0.60511725 -0.55225527]\n",
            " [-0.93297509  0.28724456  1.37184579  0.41589874  0.34981245 -0.24753755\n",
            "  -1.24497125  0.05044148]\n",
            " [-0.11414585 -0.01545749 -0.58376828 -0.40193907  0.93931836 -1.94334363\n",
            "  -0.34770465  1.50103406]]\n",
            "K\n",
            " [[ 1.1226585  -0.85645535  0.54315044  1.36560451  0.52539476 -0.94502504\n",
            "  -0.48444661  0.46268014]\n",
            " [-0.53713766 -1.16937329 -0.57988617  0.92713577 -0.85995607 -0.40352635\n",
            "   0.26555146 -1.83159914]\n",
            " [-2.06994435 -0.09514715 -1.64928361 -0.17375184  0.13146819 -1.76335363\n",
            "   1.56568846  0.69751826]\n",
            " [ 0.32910684 -0.1939204  -0.80444134  0.78816869  0.35599408  0.28309835\n",
            "  -0.25970963  1.49744622]]\n",
            "V\n",
            " [[-0.00368231  1.43739233 -0.59614565 -1.23171219  1.12030717 -0.98620738\n",
            "  -0.15461465 -1.03106383]\n",
            " [ 0.85585446 -1.79878344  0.67321704  0.05607552 -0.15542661 -1.41264124\n",
            "  -0.40136933 -1.17626611]\n",
            " [ 0.50465335  2.28693419  0.67128338  0.2506863   1.78802234  0.14775751\n",
            "  -0.11405725  0.88026286]\n",
            " [-0.68069105  0.68385101  0.17994557 -1.68013201  0.91543969 -0.19108312\n",
            "   0.03160471  1.40527326]]\n",
            "New V\n",
            " [[-0.00368231  1.43739233 -0.59614565 -1.23171219  1.12030717 -0.98620738\n",
            "  -0.15461465 -1.03106383]\n",
            " [ 0.41440401 -0.13671232  0.02128364 -0.60532081  0.49977893 -1.1936286\n",
            "  -0.27463831 -1.10169151]\n",
            " [ 0.32673907  0.72121642 -0.00947672 -0.59897862  0.90155754 -0.88535361\n",
            "  -0.21384855 -0.7053796 ]\n",
            " [ 0.18700384  1.67754576  0.33105314 -0.41795742  1.4258469  -0.18788199\n",
            "  -0.10285145  0.54683565]]\n",
            "Attention\n",
            " [[1.         0.         0.         0.        ]\n",
            " [0.51359112 0.48640888 0.         0.        ]\n",
            " [0.53753304 0.27144826 0.1910187  0.        ]\n",
            " [0.19293995 0.03256643 0.57960627 0.19488734]]\n"
          ]
        }
      ],
      "source": [
        "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)\n",
        "print(\"New V\\n\", values)\n",
        "print(\"Attention\\n\", attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HtQQtB2LJus"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "DLNLP",
      "language": "python",
      "name": "dlnlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
